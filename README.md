# GraSS: Gradient Subgroup Scanning for Distributionally and Outlier Robust Models


## Environment

Create an environment with the following commands:
```
virtualenv venv -p python3
source venv/bin/activate
pip install -r requirements.txt
```

## Downloading Dataset

**Waterbirds:** Download waterbirds from [here](https://nlp.stanford.edu/data/dro/waterbird_complete95_forest2water2.tar.gz) and put it in `grass/cub/data`.
  - In that directory, our code expects `waterbird_complete95_forest2water2/` with `metadata.csv` inside.

To get the data, run the following commands in the `grass/` directory:

```
wget -c https://nlp.stanford.edu/data/dro/waterbird_complete95_forest2water2.tar.gz
mkdir cub && mkdir cub/data/
tar -xf waterbird_complete95_forest2water2.tar.gz -C cub/data/
```

**Outliers:** Introduce 5% outliers into the dataset. The new dataset with outliers will replace `waterbird_complete95_forest2water2/`. The original unmodified dataset will be backed-up in `waterbird_original/`.
To introduce outliers into the dataset, run the following commands:

```
python generate_outliers.py
cd cub/data/
mv waterbird_complete95_forest2water2/ waterbird_original/ && mv waterbird_outliers/ waterbird_complete95_forest2water2/
```

## **Running our Method**

**Stage 1**
- Extract features:
    - `python feature_extract.py`
        - Some useful optional args: `--model_name $MODEL_NAME --extracted_dir $EXTRACTED_DIR`
        - Accepted model names: `pretrained-50`, `pretrained-18`, `fc`, `fcfc`
        - If `model_name` is `fc` or `fcfc`:
            - You must provide the path to the `.pth` file with the trained model via `--alt_model_dir $ALT_MODEL_DIR`
            - You can extract the gradients from the last layer (`fc`) or the last two layers (`fcfc`) via `--get_fc_grads`
    
- Classification model and extract gradients:
    - `python extract_grads.py`
        - Some useful optional args: `--model_name $MODEL_NAME --extracted_dir $EXTRACTED_DIR --epochs $EPOCHS`
        - Accepted model names: `pretrained-50`, `pretrained-18`
        - If `model_name` is `fc` or `fcfc`, you must extract gradients via `feature_extract.py` as described above. These gradients need to be handled manually for clustering as they are not compatible with the default args.
        
**Stage 2**
- DBSCAN cluster:
    - `python dbscan_cluster.py --epoch $EPOCH`
    - The epoch used should be the epoch identified in the names of the files generated by `extract_grads.py`. These files can be found in `extracted/`.
    
    - Some useful optional args: `--eps $EPS --min_samples $MIN_SAMPLES --model_name $MODEL_NAME --extracted_dir $EXTRACTED_DIR --out_dir $OUT_DIR`
        
**Stage 3**
- Downstream task:
    - `python generate_downstream.py --exp_name $EXPERIMENT_NAME --dataset CUB --method GROUP_DRO --exclude_outliers --override_groups_file $GROUPS_FILE_PATH`
        - Some useful optional args: `--n_epochs $EPOCHS --lr $LR --weight_decay $WD`. Other args, e.g. batch size, can be changed in generate_downstream.py.
    - Bash execute the generated script for GROUP_DRO inside `results/dataset/$EXPERIMENT_NAME`

## Monitoring Performance

- Run `python analysis.py --exp_name $EXPERIMENT_NAME --dataset CUB`
- You can also track accuracies in train.csv, val.csv, and test.csv in the directory or use wandb to monitor performance for all experiments
- Note that training and validation group accuracies are evaluated for the 'groups' which were identified in *Stage 2*. These are not the true groups and thus we do not necessarily 
expect to see high group accuracies for training and validation subsets.


## Running ERM
- Run `python generate_downstream.py --exp_name $EXPERIMENT_NAME --dataset CUB --method ERM`
        - Some useful optional args: `--n_epochs $EPOCHS --lr $LR --weight_decay $WD`
- Bash execute the generated script for the method inside `results/dataset/$EXPERIMENT_NAME`

## Sample Commands for running GraSS on Waterbirds

```
python feature_extract.py

python extract_grads.py

python dbscan_cluster.py --epoch 5 --eps 0.4 --min_samples 100

python generate_downstream.py --exp_name CUB_sample_exp --dataset CUB --n_epochs 300 --lr 1e-5 --weight_decay 1.0 --method GROUP_DRO --override_groups_file train_val_test_labels_0.4_100.csv --exclude_outliers

bash results/CUB/CUB_sample_exp/GROUP_DRO_upweight_0_epochs_300_lr_1e-05_weight_decay_1.0/job.sh

python analysis.py --exp_name CUB_sample_exp --dataset CUB
```
